"""
Agent åˆ›å»ºå’Œé…ç½®æ¨¡å—

æä¾›é¢„é…ç½®çš„ Agent å®ä¾‹ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æç¤ºè¯ç”Ÿæˆå·¥ä½œæµã€‚
"""

import os
from typing import Any, Iterator, Tuple

from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, FilesystemBackend, StateBackend, StoreBackend
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, AIMessageChunk, ToolMessage
from langchain_deepseek import ChatDeepSeek
from langfuse.langchain import CallbackHandler
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.state import CompiledStateGraph
from langgraph.store.memory import InMemoryStore
from prompt_toolkit import PromptSession

from toolkits import FileBasedPromptToolkit

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def print_stream(stream: Iterator[Tuple[str, Any]]) -> None:
    """
    ç¾åŒ–æµå¼è¾“å‡ºï¼ŒåŒºåˆ†å·¥å…·è°ƒç”¨å’Œæ™ºèƒ½ä½“å“åº”

    Args:
        stream: agent.stream() è¿”å›çš„è¿­ä»£å™¨
    """
    import sys

    for mode, chunk in stream:
        if mode == "messages":
            msg, _ = chunk  # metadata æœªä½¿ç”¨ï¼Œç”¨ _ å¿½ç•¥

            # AI æ¶ˆæ¯ï¼ˆæ™ºèƒ½ä½“æ€è€ƒè¿‡ç¨‹ï¼‰
            if isinstance(msg, (AIMessage, AIMessageChunk)):
                # æœ‰å·¥å…·è°ƒç”¨æ—¶
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        tool_name = tool_call.get("name", "").strip()  # è·å– name å¹¶å»é™¤ç©ºç™½
                        tool_args = tool_call.get("args", {})

                        # åªæœ‰å½“å·¥å…·åä¸ä¸ºç©ºæ—¶æ‰æ˜¾ç¤ºï¼ˆè¿‡æ»¤æµå¼ä¼ è¾“ä¸­çš„ç©ºå—ï¼‰
                        if tool_name:
                            print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}", file=sys.stderr)
                            if tool_args:
                                # æ ¼å¼åŒ–å‚æ•°æ˜¾ç¤º
                                args_str = ", ".join(f"{k}={v}" for k, v in tool_args.items())
                                print(f"   å‚æ•°: {args_str}", file=sys.stderr)

                # æœ‰å†…å®¹æ—¶ï¼ˆæ™ºèƒ½ä½“çš„å›å¤ï¼‰
                if hasattr(msg, "content") and msg.content:
                    print(msg.content, end="", flush=True)

            # å·¥å…·è¾“å‡ºæ¶ˆæ¯
            elif isinstance(msg, ToolMessage):
                tool_name = msg.name
                content = msg.content

                # ç®€åŒ–å·¥å…·è¾“å‡ºæ˜¾ç¤º
                if content and len(content) > 200:
                    preview = content[:200] + "..."
                else:
                    preview = content

                print(f"\nâœ… å·¥å…·å®Œæˆ: {tool_name}", file=sys.stderr)
                if preview.strip():
                    print(f"   è¾“å‡º: {preview}", file=sys.stderr)

        elif mode == "updates":
            # çŠ¶æ€æ›´æ–°ï¼ˆå¯é€‰ï¼šæ˜¾ç¤ºå·¥ä½œè¿›åº¦ï¼‰
            pass

    print("\n", file=sys.stderr)  # ç»“æŸæ¢è¡Œ


def get_deepseek_model():
    """è·å– DeepSeek æ¨¡å‹å®ä¾‹"""
    api_key = os.getenv("DEEP_SEEK_API_KEY")
    return ChatDeepSeek(api_key=api_key, model="deepseek-chat")


def create_file_based_prompt_agent(model=None, work_dir: str = "memories") -> CompiledStateGraph:
    """
    åˆ›å»ºåŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æç¤ºè¯ç”Ÿæˆ Agent

    ä½¿ç”¨æ–‡ä»¶ I/O ç‰ˆæœ¬çš„ prompt å·¥å…·åŒ…ï¼Œæ”¯æŒï¼š
    - ä¸­é—´ç»“æœå¯æŸ¥çœ‹ã€å¯ç¼–è¾‘
    - å·¥ä½œæµå¯ä¸­æ–­æ¢å¤
    - äººå·¥ä»‹å…¥è°ƒæ•´
    - å¯¹è¯å†å²è®°å¿†ï¼ˆé€šè¿‡ checkpointerï¼‰

    è·¯å¾„ç³»ç»Ÿï¼š
    - Agent å†…éƒ¨ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ "requirement.txt"ï¼‰
    - æ–‡ä»¶ç³»ç»Ÿåç«¯ï¼š/memories/workspace/ -> {work_dir}/workspace/ï¼ˆç£ç›˜æŒä¹…åŒ–ï¼‰
    - ä¸´æ—¶ç©ºé—´ï¼š/ï¼ˆå†…å­˜ï¼Œä¼šè¯ç»“æŸä¸¢å¤±ï¼‰

    Args:
        model: LangChain æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨ DeepSeek
        work_dir: ç£ç›˜æŒä¹…åŒ–æ ¹ç›®å½•ï¼ˆä¾‹å¦‚ "/home/user/code/promptx/memories"ï¼‰

    Returns:
        é…ç½®å¥½çš„ deep agent
    """
    if model is None:
        model = get_deepseek_model()

    # æ„å»ºçœŸå®ç£ç›˜è·¯å¾„
    if work_dir.startswith("/"):
        real_work_dir = work_dir
    else:
        real_work_dir = f"/home/zhonghan.chen/code/promptx/{work_dir}"

    prompt_toolkit = FileBasedPromptToolkit(model=model, work_dir=real_work_dir)

    # åˆ›å»º checkpointer å®ç°å¯¹è¯è®°å¿†
    checkpointer = MemorySaver()

    agent = create_deep_agent(
        name="file-prompt-agent",
        model=model,
        tools=prompt_toolkit.get_tools(),
        store=InMemoryStore(),
        backend=lambda rt: CompositeBackend(
            default=StateBackend(rt),
            routes={
                "/memories/": FilesystemBackend(
                    root_dir=real_work_dir,
                    virtual_mode=True,  # å¯ç”¨è™šæ‹Ÿæ¨¡å¼ï¼Œå®‰å…¨é™åˆ¶åœ¨ç›®å½•å†…
                ),
            },
        ),
        checkpointer=checkpointer,  # å¯ç”¨å¯¹è¯å†å²è®°å¿†
        system_prompt=f"""ä½ æ˜¯ä¸€ä¸ªæç¤ºè¯ç”Ÿæˆä¸“å®¶åŠ©æ‰‹ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„çŠ¶æ€æœºå·¥ä½œæµã€‚

   ## æ ¸å¿ƒè§’è‰²
   æç¤ºè¯ç”Ÿæˆä¸“å®¶åŠ©æ‰‹ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„çŠ¶æ€æœºå·¥ä½œæµã€‚

   ## å·¥ä½œç›®å½•
   /memories/workspace/ï¼ˆæŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰

   ## æ ‡å‡†æµç¨‹
   1. å‡†å¤‡éœ€æ±‚ â†’ requirement.txt
   2. ç”Ÿæˆè§„æ ¼ â†’ prompt_architect_file() â†’ analysis.json
   3. ç”Ÿæˆæµ‹è¯• â†’ data_generator_file() â†’ test_data.json
   4. ç”Ÿæˆæç¤º â†’ prompt_builder_file() â†’ final_prompt.json

   ## äº¤äº’åŸåˆ™
   - å…ˆäº¤æµç†è§£éœ€æ±‚ï¼Œå†æ‰§è¡Œ
   - æŒ‰æ­¥éª¤æ‰§è¡Œï¼Œå±•ç¤ºä¸­é—´ç»“æœ
   - æ ¹æ®åé¦ˆçµæ´»è°ƒæ•´
""",
    )

    return agent


if __name__ == "__main__":
    deepseek = get_deepseek_model()
    agent = create_file_based_prompt_agent(model=deepseek)

    # åˆ›å»º prompt_toolkit ä¼šè¯
    session = PromptSession("ğŸ’¬ ä½ : ")

    print("ğŸ¤– æç¤ºè¯ç”ŸæˆåŠ©æ‰‹å·²å¯åŠ¨ï¼è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º\n")

    # äº¤äº’å¼å¯¹è¯å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥ï¼ˆä½¿ç”¨ prompt_toolkitï¼Œæ”¯æŒä¸­æ–‡æ­£ç¡®åˆ é™¤ï¼‰
            user_input = session.prompt().strip()

            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
                print("ğŸ‘‹ å†è§ï¼")
                break

            # è·³è¿‡ç©ºè¾“å…¥
            if not user_input:
                continue

            # æ‰§è¡Œ agent æµå¼è¾“å‡º
            print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            stream = agent.stream(
                input={"messages": [{"role": "user", "content": user_input}]},
                config={"callbacks": [CallbackHandler()], "configurable": {"thread_id": "test_session"}},
                stream_mode=["messages"],
            )

            # ä½¿ç”¨ç¾åŒ–æ‰“å°å‡½æ•°
            print_stream(stream)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except EOFError:
            # Ctrl+D é€€å‡º
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            continue
