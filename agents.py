"""
Agent åˆ›å»ºå’Œé…ç½®æ¨¡å—

æä¾›é¢„é…ç½®çš„ Agent å®ä¾‹ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æç¤ºè¯ç”Ÿæˆå·¥ä½œæµã€‚
"""

import os
from typing import Any, Iterator, Tuple

from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, FilesystemBackend, StateBackend
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, AIMessageChunk, ToolMessage
from langchain_deepseek import ChatDeepSeek
from langfuse.langchain import CallbackHandler
from langgraph.store.memory import InMemoryStore

from toolkits import FileBasedPromptToolkit

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def print_stream(stream: Iterator[Tuple[str, Any]]) -> None:
    """
    ç¾åŒ–æµå¼è¾“å‡ºï¼ŒåŒºåˆ†å·¥å…·è°ƒç”¨å’Œæ™ºèƒ½ä½“å“åº”

    Args:
        stream: agent.stream() è¿”å›çš„è¿­ä»£å™¨
    """
    import sys

    for mode, chunk in stream:
        if mode == "messages":
            msg, metadata = chunk

            # AI æ¶ˆæ¯ï¼ˆæ™ºèƒ½ä½“æ€è€ƒè¿‡ç¨‹ï¼‰
            if isinstance(msg, (AIMessage, AIMessageChunk)):
                # æœ‰å·¥å…·è°ƒç”¨æ—¶
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        tool_name = tool_call.get("name", "").strip()  # è·å– name å¹¶å»é™¤ç©ºç™½
                        tool_args = tool_call.get("args", {})

                        # åªæœ‰å½“å·¥å…·åä¸ä¸ºç©ºæ—¶æ‰æ˜¾ç¤ºï¼ˆè¿‡æ»¤æµå¼ä¼ è¾“ä¸­çš„ç©ºå—ï¼‰
                        if tool_name:
                            print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}", file=sys.stderr)
                            if tool_args:
                                # æ ¼å¼åŒ–å‚æ•°æ˜¾ç¤º
                                args_str = ", ".join(f"{k}={v}" for k, v in tool_args.items())
                                print(f"   å‚æ•°: {args_str}", file=sys.stderr)

                # æœ‰å†…å®¹æ—¶ï¼ˆæ™ºèƒ½ä½“çš„å›å¤ï¼‰
                if hasattr(msg, "content") and msg.content:
                    print(msg.content, end="", flush=True)

            # å·¥å…·è¾“å‡ºæ¶ˆæ¯
            elif isinstance(msg, ToolMessage):
                tool_name = msg.name
                content = msg.content

                # ç®€åŒ–å·¥å…·è¾“å‡ºæ˜¾ç¤º
                if content and len(content) > 200:
                    preview = content[:200] + "..."
                else:
                    preview = content

                print(f"\nâœ… å·¥å…·å®Œæˆ: {tool_name}", file=sys.stderr)
                if preview.strip():
                    print(f"   è¾“å‡º: {preview}", file=sys.stderr)

        elif mode == "updates":
            # çŠ¶æ€æ›´æ–°ï¼ˆå¯é€‰ï¼šæ˜¾ç¤ºå·¥ä½œè¿›åº¦ï¼‰
            pass

    print("\n", file=sys.stderr)  # ç»“æŸæ¢è¡Œ


def get_deepseek_model():
    """è·å– DeepSeek æ¨¡å‹å®ä¾‹"""
    api_key = os.getenv("DEEP_SEEK_API_KEY")
    return ChatDeepSeek(api_key=api_key, model="deepseek-chat")


def create_file_based_prompt_agent(model=None, work_dir: str = "memories") -> Any:
    """
    åˆ›å»ºåŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æç¤ºè¯ç”Ÿæˆ Agent

    ä½¿ç”¨æ–‡ä»¶ I/O ç‰ˆæœ¬çš„ prompt å·¥å…·åŒ…ï¼Œæ”¯æŒï¼š
    - ä¸­é—´ç»“æœå¯æŸ¥çœ‹ã€å¯ç¼–è¾‘
    - å·¥ä½œæµå¯ä¸­æ–­æ¢å¤
    - äººå·¥ä»‹å…¥è°ƒæ•´

    è·¯å¾„ç³»ç»Ÿï¼š
    - Agent å†…éƒ¨ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ "requirement.txt"ï¼‰
    - æ–‡ä»¶ç³»ç»Ÿåç«¯ï¼š/memories/workspace/ -> {work_dir}/workspace/ï¼ˆç£ç›˜æŒä¹…åŒ–ï¼‰
    - ä¸´æ—¶ç©ºé—´ï¼š/ï¼ˆå†…å­˜ï¼Œä¼šè¯ç»“æŸä¸¢å¤±ï¼‰

    Args:
        model: LangChain æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨ DeepSeek
        work_dir: ç£ç›˜æŒä¹…åŒ–æ ¹ç›®å½•ï¼ˆä¾‹å¦‚ "/home/user/code/promptx/memories"ï¼‰

    Returns:
        é…ç½®å¥½çš„ deep agent
    """
    if model is None:
        model = get_deepseek_model()

    # æ„å»ºçœŸå®ç£ç›˜è·¯å¾„
    if work_dir.startswith("/"):
        real_work_dir = work_dir
    else:
        real_work_dir = f"/home/zhonghan.chen/code/promptx/{work_dir}"

    prompt_toolkit = FileBasedPromptToolkit(model=model, work_dir=real_work_dir)

    agent = create_deep_agent(
        name="file-prompt-agent",
        model=model,
        tools=prompt_toolkit.get_tools(),
        store=InMemoryStore(),
        backend=lambda rt: CompositeBackend(
            default=StateBackend(rt),
            routes={
                "/memories/": FilesystemBackend(
                    root_dir=real_work_dir,
                    virtual_mode=True,  # å¯ç”¨è™šæ‹Ÿæ¨¡å¼ï¼Œå®‰å…¨é™åˆ¶åœ¨ç›®å½•å†…
                ),
            },
        ),
        system_prompt=f"""ä½ æ˜¯ä¸€ä¸ªæç¤ºè¯ç”Ÿæˆä¸“å®¶ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„çŠ¶æ€æœºå·¥ä½œæµã€‚

## çŠ¶æ€æœºå·¥ä½œæµ

å·¥ä½œç›®å½•ï¼š`/memories/workspace/`ï¼ˆæŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰

æ ‡å‡†æµç¨‹ï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰ï¼š

**æ­¥éª¤ 1ï¼šå‡†å¤‡éœ€æ±‚**
```
write_file("requirement.txt", "<ç”¨æˆ·éœ€æ±‚>")
```

**æ­¥éª¤ 2ï¼šç”ŸæˆæŠ€æœ¯è§„æ ¼**
```
prompt_architect_file()
```
â†’ è¯»å– requirement.txt â†’ å†™å…¥ analysis.json

**æ­¥éª¤ 3ï¼šç”Ÿæˆæµ‹è¯•æ•°æ®**
```
data_generator_file(num=5)
```
â†’ è¯»å– analysis.json â†’ å†™å…¥ test_data.json

**æ­¥éª¤ 4ï¼šç”Ÿæˆæœ€ç»ˆæç¤ºè¯**
```
prompt_builder_file()
```
â†’ è¯»å– analysis.json + test_data.json â†’ å†™å…¥ final_prompt.json

## å·¥å…·è¯´æ˜

### æç¤ºè¯å·¥ç¨‹å·¥å…·ï¼ˆæ— å‚æ•°ï¼‰
- `prompt_architect_file()` - ç”ŸæˆæŠ€æœ¯è§„æ ¼
- `data_generator_file(num=3)` - ç”Ÿæˆæµ‹è¯•æ•°æ®
- `prompt_builder_file()` - ç”Ÿæˆæœ€ç»ˆæç¤ºè¯

### è¾…åŠ©å·¥å…·
- `ls`ã€`read_file`ã€`write_file`ã€`edit_file` - æŸ¥çœ‹å’Œç¼–è¾‘æ–‡ä»¶
- `web_search`ã€`web_reader` - è”ç½‘æœç´¢å’Œé˜…è¯»

## å·¥ä½œåŸåˆ™

1. **å…¨è‡ªåŠ¨æ‰§è¡Œ**ï¼šæ— éœ€ä¸ç”¨æˆ·äº¤æµï¼Œç›´æ¥ä½¿ç”¨å·¥å…·å®Œæˆä»»åŠ¡
2. **æŒ‰é¡ºåºè°ƒç”¨**ï¼šä¸¥æ ¼æŒ‰ç…§æ­¥éª¤ 1â†’2â†’3â†’4 æ‰§è¡Œ
3. **æ£€æŸ¥ä¸­é—´ç»“æœ**ï¼šæ¯æ­¥å®Œæˆåå¯ç”¨ `read_file` æŸ¥çœ‹è¾“å‡º
4. **çµæ´»è°ƒæ•´**ï¼šå¦‚å‘ç°é—®é¢˜å¯ç”¨ `edit_file` ä¿®æ”¹åç»§ç»­

## ç£ç›˜æ˜ å°„

`/memories/workspace/` â†’ `{real_work_dir}/workspace/`
""",
    )

    return agent


if __name__ == "__main__":
    deepseek = get_deepseek_model()
    agent = create_file_based_prompt_agent(model=deepseek)

    # æµ‹è¯•æµå¼è¾“å‡º
    user_input = "è¯·å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªæç¤ºè¯ï¼Œæˆ‘å°†è§†é¢‘é€šè¿‡ASRè½¬æ¢ä¸ºæ–‡æœ¬ã€‚ä½†æ˜¯æ–‡æœ¬ä¸­æœ‰å¾ˆå¤šè¯†åˆ«é”™è¯¯ã€‚è¯·å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªæç¤ºè¯ï¼Œèƒ½å¸®æˆ‘çº æ­£è¿™äº›ASRé”™è¯¯ã€‚å¹¶æ¢³ç†æˆä¸€ä¸ªè§†é¢‘å†…å®¹çš„æ€»ç»“æŠ¥å‘Š,è¾“å‡ºä¸ºMarkdownæ ¼å¼çš„æ–‡æœ¬å³å¯,ä¸éœ€è¦ç»“æ„åŒ–æ•°æ®ã€‚,è¾“å…¥ä¹Ÿå°±ä¸€ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²"
    stream = agent.stream(
        input={"messages": [{"role": "user", "content": user_input}]},
        config={"callbacks": [CallbackHandler()]},
        stream_mode=["messages"],
    )

    # ä½¿ç”¨ç¾åŒ–æ‰“å°å‡½æ•°
    print_stream(stream)
